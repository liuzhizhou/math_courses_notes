\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{Bertsekas/99}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Preliminaries}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Derivatives}{3}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Fundamental Concepts}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convexity}{5}{section.2.1}\protected@file@percent }
\newlabel{eq:convex}{{2.1.1}{5}{convex set, convex function}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Convex Hull and Affine Hull}{6}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Fundamental Terminologies}{6}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}General Optimality Condition}{7}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Unconstrained Optimality Conditions}{8}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}General Case}{8}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Convex Case}{9}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Sufficient Conditions}{10}{subsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Algorithms: Gradient Methods}{10}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Descent Direction}{10}{subsection.2.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces  Various choice of the positive definite matrix $D^k$, where $d^k=-D^k \nabla f(x^k)$. Gauss Newton Method is widely used when the cost function $f(x)$ is of the form $f(x)=\frac  {1}{2}\left \lVert g(x) \right \rVert ^2=\frac  {1}{2}\DOTSB \sum@ \slimits@ _{i=1}^m (g_i(x))^2,$ where $g=(g_1,\dots  ,g_m)$, which is a problem often encountered in statistical data analysis and in the context of neural network training.}}{11}{table.2.1}\protected@file@percent }
\newlabel{table:descent direction}{{2.1}{11}{Various choice of the positive definite matrix $D^k$, where $d^k=-D^k \grad f(x^k)$. Gauss Newton Method is widely used when the cost function $f(x)$ is of the form $f(x)=\frac {1}{2}\norm {g(x)}^2=\frac {1}{2}\sum _{i=1}^m (g_i(x))^2,$ where $g=(g_1,\dots ,g_m)$, which is a problem often encountered in statistical data analysis and in the context of neural network training}{table.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Various choice of stepsize $\alpha ^k$. In Arimijo rule, first choose fix scalars $s,\beta $ and $\sigma $, with $0<\beta <1$ and $0<\sigma <1$, let $m_k$ be the first non-negative integer $m$ such that $f(x^k)-f(x^k+\beta ^m s d^k)\geq -\sigma \beta ^m s \nabla f(x^k)^\top d^k$. Note that here $\beta ^m$ means $\beta $ to the $m$-th power. In diminishing method, we require $\DOTSB \sum@ \slimits@ _{k=0}^{\infty }\alpha ^k=\infty $ to guarantees that $\{x^k\}$ does not converge to a non-stationary point. Indeed, if $x^k\to \bar  {x}$, then for large $m,n$, $x^m\approx x^n \approx \bar  {x}$, also $x^m\approx x^n -(\DOTSB \sum@ \slimits@ _{k=n}^{m-1}\alpha ^k)\nabla f(\bar  {x})$, which shows $\nabla f(\bar  {x})$ must be zero.}}{11}{table.2.2}\protected@file@percent }
\newlabel{table:stepsize}{{2.2}{11}{Various choice of stepsize $\alpha ^k$. In Arimijo rule, first choose fix scalars $s,\beta $ and $\sigma $, with $0<\beta <1$ and $0<\sigma <1$, let $m_k$ be the first non-negative integer $m$ such that $f(x^k)-f(x^k+\beta ^m s d^k)\geq -\sigma \beta ^m s \grad f(x^k)^\T d^k$. Note that here $\beta ^m$ means $\beta $ to the $m$-th power. In diminishing method, we require $\sum _{k=0}^{\infty }\alpha ^k=\infty $ to guarantees that $\{x^k\}$ does not converge to a non-stationary point. Indeed, if $x^k\to \bar {x}$, then for large $m,n$, $x^m\approx x^n \approx \bar {x}$, also $x^m\approx x^n -(\sum _{k=n}^{m-1}\alpha ^k)\grad f(\bar {x})$, which shows $\grad f(\bar {x})$ must be zero}{table.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Stepsize}{11}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Mathematical Statements for Convergence Results}{11}{subsection.2.6.3}\protected@file@percent }
\newlabel{eq:constant stepsize condition}{{2.6.4}{12}{Constant Stepsize}{equation.2.6.4}{}}
\bibstyle{alpha}
\bibdata{opt_bib.bib}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Rate of Convergence}{14}{subsection.2.6.4}\protected@file@percent }
\bibcite{Bertsekas/99}{Ber99}
\gdef \@abspage@last{16}
