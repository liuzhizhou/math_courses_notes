\documentclass[12pt,a4paper]{article}

\usepackage[top=3cm,bottom=3cm,left=3cm,right=3cm]{geometry}
%% Packages
%% ========

%% LaTeX Font encoding -- DO NOT CHANGE
\usepackage[OT1]{fontenc}

%% Babel provides support for languages.  'english' uses British
%% English hyphenation and text snippets like "Figure" and
%% "Theorem". Use the option 'ngerman' if your document is in German.
%% Use 'american' for American English.  Note that if you change this,
%% the next LaTeX run may show spurious errors.  Simply run it again.
%% If they persist, remove the .aux file and try again.
\usepackage[english]{babel}

%% Input encoding 'utf8'. In some cases you might need 'utf8x' for
%% extra symbols. Not all editors, especially on Windows, are UTF-8
%% capable, so you may want to use 'latin1' instead.
\usepackage[utf8]{inputenc}

%% This changes default fonts for both text and math mode to use Herman Zapfs
%% excellent Palatino font.  Do not change this.
%\usepackage[sc]{mathpazo}

%% The AMS-LaTeX extensions for mathematical typesetting.  Do not
%% remove.
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathrsfs}

\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}

%% LaTeX' own graphics handling
\usepackage{graphicx}

%% This allows you to add .pdf files. It is used to add the
%% declaration of originality.
\usepackage{pdfpages}

\usepackage{mathtools}

\numberwithin{equation}{section}

\newtheoremstyle{mystyle}% ⟨name ⟩ 
{3pt}% ⟨Space above ⟩1 
{3pt}% ⟨Space below ⟩1
{}% ⟨Body font ⟩
{}% ⟨Indent amount ⟩2
{\sffamily}% ⟨Theorem head font⟩
{.}% ⟨Punctuation after theorem head ⟩
{.5em}% ⟨Space after theorem head ⟩3
{}% ⟨Theorem head spec (can be left empty, meaning ‘normal’)⟩
%
%
%\newtheoremstyle{break}%
%{}{}%
%{}{}%
%{\bfseries}{}%  % Note that final punctuation is omitted.
%{\newline}{}

\theoremstyle{mystyle}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{example}[definition]{Example}
%\tcbuselibrary{theorems}
\tcbuselibrary{skins,breakable}



\tcolorboxenvironment{theorem}{
	enhanced jigsaw,colframe=Salmon!90!Black,interior hidden, breakable,before skip=10pt,after skip=10pt 
}

\tcolorboxenvironment{proposition}{
	blanker,breakable,left=5mm,
	before skip=10pt,after skip=10pt,
	borderline west={1mm}{0pt}{Green!70}
}

\tcolorboxenvironment{definition}{
	blanker,breakable,left=5mm,
	before skip=10pt,after skip=10pt,
	borderline west={1mm}{0pt}{cyan!40!black}
}

\tcolorboxenvironment{example}{
	blanker,breakable,left=5mm,
	before skip=10pt,after skip=10pt,
	borderline west={1mm}{0pt}{green!35!black}
}

\tcolorboxenvironment{lemma}{
	blanker,breakable,left=5mm,
	before skip=10pt,after skip=10pt,
	borderline west={1mm}{0pt}{RoyalPurple!55!Aquamarine!100!}
}

\tcolorboxenvironment{corollary}{
	blanker,breakable,left=5mm,
	before skip=10pt,after skip=10pt,
	borderline west={1mm}{0pt}{CornflowerBlue!60!Black}
}




\tcolorboxenvironment{proof}{% `proof' from `amsthm' 
	blanker,breakable,right=5mm,
	before skip=10pt,after skip=10pt,
	borderline east={0.5mm}{1pt}{red!10!white}}

\usepackage[linkcolor=blue,colorlinks=cyan,citecolor=red,filecolor=black]{hyperref}

\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	linkcolor=NavyBlue,
	citecolor=OrangeRed,
	filecolor=orange}

\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{cd}



\title{Notes on Optimization}
\author{Liu Zhizhou}
\date{First Created: August 6, 2022\\
	Last Modified: \today}


\newcommand{\R}{\mathbb{R}}
\newcommand{\grad}{\nabla}
\renewcommand{\d}{\mathrm{d}}

\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\inner}[1]{\left\langle #1 \right\rangle}
\begin{document}
	{\sffamily \maketitle}
	
	
	\tableofcontents
	
	\section{Derivatives}
	Let $f:\R^n \to \R$. The \emph{gradient} of $f$ at $x$ is defined as the column vector
	$$
	\grad f(x)=
	\begin{bmatrix}
		\frac{\partial f(x)}{\partial x_1}\\
		\vdots\\
		\frac{\partial f(x)}{\partial x_n}
	\end{bmatrix}.
	$$
	If $f$ is a vector-valued function, i.e. $f:\R^n\to \R^m$, with component functions $f_1,\dots,f_m$, then
	$$
	\grad f(x)=
	\begin{bmatrix}
		\grad f_1(x) & \cdots & \grad f_m(x)
	\end{bmatrix}.
	$$
	The transpose of $\grad f$ is called the \emph{Jacobian} of $f$. The Jacobian of $f$ is the matrix whose $ij$-th entry is equal to the partial derivative $\frac{\partial f_i}{\partial x_j}$.
	
	The \emph{Hessian} of $f:\R^n\to \R$ is the matrix whose $ij$-th entry is equal to $\frac{\partial^2 f}{\partial x_i \partial x_j}$, denoted by $\grad^2 f$.
	
	Be careful that, for $f:\R^n \to \R$, $\grad^2 f\neq \grad(\grad f)$, but $\grad^2 f = \grad(\grad f^T)$.
	\begin{proposition}[chain rule]
		Let $f:\R^k\to \R^m$ and $g:\R^m\to\R^n$ be smooth functions, and $h=g(f(x))$. Then
		$$
		\grad h(x) = \grad f(x)\grad(g(f(x)))
		$$
		for all $x\in \R^k$.
	\end{proposition}
	Some useful relations:
	\begin{enumerate}
		\item $\grad (Ax)=A^T$;
		\item $\grad (x^T A x)=(A+A^T)x$; in particular, if $Q$ is symmetric, then $\grad(x^T Q x)=2Qx$ and $\grad(\norm{x}^2)=\grad(x^T x)=2x$;
		\item $\grad(f(Ax))=A^T \grad f(Ax)$;
		\item $\grad^2(f(Ax))=A^T \grad^2 f(Ax)A$;
	\end{enumerate}
	The shape of the left hand side would be helpful to memorize the right hand side.
	
	\begin{theorem}[Second Order Taylor Expansions]
		Let $f:\R^n \to \R$ be twice continuously differentiable over an open sphere $S$ centered at a vector $x$. Then for all $d$ such that $x+d\in S$,
		\begin{enumerate}
			\item we have
			$$
			f(x+d)=f(x)+d^T \grad f(x) +\frac{1}{2}d^T\left(\int_0^1\left(\int_0^t \grad^2 f(x+\tau d)\d \tau\right)\d t\right)d.
			$$
			\item there exists
			$$
			f(x+d)=f(x)+d^T \grad f(x)+\frac{1}{2}d^T \grad^2 f(x+\alpha d)d.
			$$
			\item there holds
			$$
			f(x+d)=f(x)+d^T \grad f(x)+\frac{1}{2}d^T \grad^2 f(x)d+o(\norm{d}^2).
			$$
		\end{enumerate}
	\end{theorem}


	\section{Convexity}
	\begin{definition}[convex set, convex function]
		A subset $C$ of $\R^n$ is called \emph{convex} if 
		$$
		\alpha x+(1-\alpha)y\in C
		$$ for all $x,y\in C$ and $\alpha\in [0,1]$. A function $f:C\to \R$ is called \emph{convex} if 
		\begin{equation}
			f(\alpha x+(1-\alpha )y)\leq 
			\alpha f(x)+(1-\alpha)f(y)\label{eq:convex}
		\end{equation}
		for $x,y\in C$ and $\alpha\in [0,1]$. The function is called \emph{concave} if $-f$ is convex.
	\end{definition}
	\begin{definition}[strictly convex]
		The function $f$ is called \emph{strictly convex} if Eq.(\ref{eq:convex}) is strict for all $x\neq y$ and $\alpha\in(0,1)$.
	\end{definition}

	\begin{proposition}[First Derivative Characterizations]
		Let $C$ be a convex subset of $\R^n$ and let $f:\R^n \to \R$ be differentiable over $\R^n$. Then
		\begin{enumerate}
			\item $f$ is convex over $C$ if and only if
			\begin{equation}
				f(z)\geq f(x)+(z-x)^T \grad f(x)
			\end{equation}
			for all $x,z\in C$.
			\item $f$ is strictly convex over $C$ if and only if the above inequality is strict whenever $x\neq z$.
		\end{enumerate}
	\end{proposition}

	
	\begin{definition}[strongly convex]
		A function $f:\R^n \to \R$ is called \emph{strongly convex} if for some $\sigma>0$, we have
		\begin{equation}
			f(y)\geq f(x)+\grad f(x)^T (y-x)+\frac{\sigma}{2}\norm{x-y}^2
		\end{equation}
		for all $x,y\in \R^n$.
	\end{definition}
	It can be shown that an equivalent definition is that
	\begin{equation}
		(\grad f(x)-\grad f(y))^T(x-y)\geq \sigma \norm{x-y}^2
	\end{equation}
	for all $x,y\in \R^n$.
	
	

	\section{Main Optimality Conditions}
	\begin{theorem}[Necessary Optimality Conditions]
		Let $x^*$ be an unconstrained local minimum of $f:\R^n \to \R$, and assume that $f$ is continuously differentiable in an open set $S$ containing $x^*$. Then we have the \emph{First Order Necessary Condition}:
		\begin{equation}
			\grad f(x^*)=0.
		\end{equation}
		If in addition $f$ is twice continuously differentiable within $S$, then we have the \emph{Second Order Necessary Condition}:
		\begin{equation}
			\grad^2 f(x^*)\succeq 0.
		\end{equation}
	\end{theorem}
	The intuition of this theorem is considering
	$$
	f(x^*+\Delta x)-f(x^*)\approx \grad f(x^*)^T \Delta x,
	$$
	and similarly for second order,
	$$
	f(x^*+\Delta x)-f(x^*)\approx \grad f(x^*)^T \Delta x +\frac{1}{2}\Delta x^T \grad^2 f(x^*)\Delta x.
	$$
	Read rigorous proof to see the reason.
	\begin{proof}
		Fix some $d\in \R^n$. Consider $g(\alpha)\triangleq f(x^*+\alpha d)$. Then
		$$
		0\leq 
		\lim_{\alpha\to 0}\frac{f(x^*+\alpha d)-f(x^*)}{\alpha} 
		= \frac{\d g}{\d \alpha}(0)=d^T \grad f(x^*).
		$$
		The "$\leq$" is because $x^*$ is the local minimum. Replace $d$ by $-d$, then it must be $\grad f(x^*)=0$.
		
		Assume $f$ is twice differentiable. Then the second order expansion of $g(\alpha)$ in $\alpha=0$ yields
		$$
		g(\alpha) = g(0) + \frac{\d g}{\d \alpha}(0)\alpha +
		\frac{1}{2} \frac{\d^2 g}{\d \alpha^2}(0) \alpha^2 + o(\alpha^2).
		$$
		Equivalently,
		$$
		f(x^*+\alpha d) - f(x^*) = d^T \grad f(x^*)\alpha + \frac{\alpha^2}{2} d^T \grad^2 f(x^*) d + o(\alpha^2).
		$$
		Since $\grad f(x^*)=0$, for $\alpha$ positive and near 0, we have
		$$
		0\leq \frac{f(x^*+\alpha d)-f(x^*)}{\alpha^2}= 
		\frac{1}{2} d^T \grad^2 f(x^*) d + \frac{o(\alpha^2)}{\alpha^2}.
		$$
		Then let $\alpha\to 0$, we obtain $d^T \grad^2 f(x^*) d\geq 0$, which means $\grad^2 f(x^*) \succeq 0$.
	\end{proof}
	
	\begin{proposition}
		If $X$ is a convex subset of $\R^n$ and $f:\R^n \to \R$ is convex over $X$, then a local minimum of $f$ is also a global minimum. If in addition $f$ is strictly convex over $X$, then $f$ has at most one global minimum over $X$. Moreover, if $f$ is strongly convex and $X$ is closed, then $f$ has a unique global minimum over $X$.
	\end{proposition}
	
	\begin{theorem}[Convex Case - Necessary and Sufficient Conditions]
		Let $X$ be a convex set and let $f:\R^n \to \R$ be a convex function over $X$. Then
		\begin{enumerate}
			\item If $f$ is continuously differentiable, then
			$$
			\grad f(x^*)^T(x-x^*)\geq 0
			$$
			for all $x\in X$ is a necessary and sufficient condition for $x^*$ to be a global minimum of $f$ over $X$.
			\item If $X$ is open and $f$ is continuously differentiable over $X$, then $\grad f(x^*)=0$ is a necessary and sufficient condition for $x^*$ to be a global minimum of $f$ over $X$.
		\end{enumerate}
	\end{theorem}
	Note that in the second statement, we require $X$ to be open.

	The intuition of this theorem is also
	$$
	f(x^*+\Delta x)-f(x^*)\approx \grad f(x^*)^T \Delta x.
	$$
	The proof of this need the first order characterization of convexity,
	$$
	f(x)\geq f(x^*) +\grad f(x^*)^T (x-x^*)
	$$
	for all $x\in X$.
	
	A geometric illustration of $\grad f(x^*)^T(x-x^*)$ is that: $\grad f(x^*)$ is the direction that $f$ increase the most, the condition means that the connection of $x^*$ and all feasible points $x$ in $X$ has angle less than $\frac{\pi}{2}$ with the gradient; in other words, all the direction makes $f$ increase.
	
	\begin{theorem}[Second Order Sufficient Optimality Conditions]
		Let $f:\R^n \to \R$ be twice continuously differentiable over an open set $S$. Suppose that a vector $x^*\in S$ satisfies the conditions: (i) $\grad f(x^*)=0$ and (ii) $\grad^2 f(x^*)\succ 0$. Then $x^*$ is a strict unconstrained local minimum of $f$. In particular, there exists scalars $\gamma>0$ and $\epsilon>0$ such that
		$$
		f(x)\geq f(x^*)+\frac{\gamma}{2}\norm{x-x^*}^2
		$$
		for all $\norm{x-x^*}<\epsilon$.
	\end{theorem}
	\begin{proof}
		Denote $\lambda$ the smallest eigenvalue of $\grad^2 f(x^*)$. Since $\grad^2 f(x^*)\succ 0$, $\lambda >0$. We have $d^T \grad^2 f(x^*)d \geq \lambda \norm{d}^2$ for all $d\in \R^n$. By the second order Taylor expansion
		\begin{align*}
			f(x^*+d)-f(x^*)&=\grad f(x^*)^T d + \frac{1}{2} d^T \grad^2 f(x^*)d+o(\norm{d}^2)\\
			&\geq \frac{\lambda}{2}\norm{d}^2+o(\norm{d}^2)\\
			&=\left(\frac{\lambda}{2}+\frac{o(\norm{d}^2)}{\norm{d}^2}\right)\norm{d}^2.
		\end{align*}
		Then choose $\epsilon>0$ and $\gamma>0$ such that for $\norm{d}<\epsilon$,
		$$
		\frac{\lambda}{2}+\frac{o(\norm{d}^2)}{\norm{d}^2}\geq \frac{\gamma}{2}.
		$$
		Then the proof is complete.
	\end{proof}
	\appendix
	\bibliographystyle{alpha}
	\bibliography{references} 
\end{document}
